{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ZjXH9eaILl8"
   },
   "outputs": [],
   "source": [
    "# model.py -> resnet.py (use v2) -> fcn-12.3.1.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "from pathlib import Path\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o8Qi9IY3X7L7"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ic1F439hNZlb"
   },
   "source": [
    "### `resnet_layer` (Basic Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n-giBPPpy2jD"
   },
   "outputs": [],
   "source": [
    "class resnet_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 num_filters=16,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 activation='relu',\n",
    "                 batch_normalization=True,\n",
    "                 conv_first=True):\n",
    "        super(resnet_layer, self).__init__()\n",
    "\n",
    "        self.batch_normalization = batch_normalization\n",
    "        self.conv_first = conv_first\n",
    "\n",
    "        out_channels = num_filters\n",
    "        padding = kernel_size // 2      # 'same' padding\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              out_channels,\n",
    "                              kernel_size,\n",
    "                              stride=strides,\n",
    "                              padding=padding)\n",
    "\n",
    "        # 'he_normal' initialization\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "\n",
    "        # l2 regularization\n",
    "        # not implemented\n",
    "\n",
    "        # batch normalization\n",
    "        self.batchnorm_first = nn.BatchNorm2d(in_channels)\n",
    "        self.batchnorm_last = nn.BatchNorm2d(num_filters)\n",
    "\n",
    "        # activation (only 'relu' is implemented)\n",
    "        if activation:\n",
    "            self.activation = nn.ReLU()\n",
    "        else:\n",
    "            self.activation = None\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.conv_first:\n",
    "            x = self.conv(x)\n",
    "            if self.batch_normalization:\n",
    "                x = self.batchnorm_last(x)\n",
    "            if self.activation is not None:\n",
    "                x = self.activation(x)\n",
    "        else:\n",
    "            if self.batch_normalization:\n",
    "                x = self.batchnorm_first(x)\n",
    "            if self.activation is not None:\n",
    "                x = self.activation(x)\n",
    "            x = self.conv(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iS47ri33cYjm"
   },
   "source": [
    "### `conv_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QD5t-c7ncYRs"
   },
   "outputs": [],
   "source": [
    "class conv_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,#input_shape,\n",
    "                 filters=32,\n",
    "                 kernel_size=3,\n",
    "                 strides=1,\n",
    "                 use_maxpool=True,\n",
    "                 postfix=None,          # not implemented\n",
    "                 activation=None):\n",
    "        super(conv_layer, self).__init__()\n",
    "\n",
    "        padding = kernel_size // 2      # 'same' padding\n",
    "        self.conv = nn.Conv2d(in_channels,\n",
    "                              filters,\n",
    "                              kernel_size,\n",
    "                              stride=strides,\n",
    "                              padding=padding)\n",
    "        # 'he_normal' initialization\n",
    "        nn.init.kaiming_normal_(self.conv.weight)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(filters)\n",
    "\n",
    "        self.use_maxpool = use_maxpool\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        if self.use_maxpool:\n",
    "            x = self.maxpool(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "etmML44a0Uws"
   },
   "source": [
    "### `tconv_layer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CWW4u0LT0Wpb"
   },
   "outputs": [],
   "source": [
    "class tconv_layer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 filters=32,\n",
    "                 kernel_size=3,\n",
    "                 strides=2,\n",
    "                 postfix=None):\n",
    "        super(tconv_layer, self).__init__()\n",
    "\n",
    "        padding = kernel_size // 2                  # 'same' padding\n",
    "        output_padding = kernel_size - 2*padding    # for odd paddings\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels,\n",
    "                                                 filters,\n",
    "                                                 kernel_size,\n",
    "                                                 stride=strides,\n",
    "                                                 padding=padding,\n",
    "                                                 output_padding=output_padding)\n",
    "        # 'he_normal' initialization\n",
    "        nn.init.kaiming_normal_(self.conv_transpose.weight)\n",
    "\n",
    "        self.batchnorm = nn.BatchNorm2d(filters)\n",
    "        self.activation = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xgpEV581aX3H"
   },
   "source": [
    "### `features_pyramid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wl1M9ewzaXlt"
   },
   "outputs": [],
   "source": [
    "class features_pyramid(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels,\n",
    "                 n_layers):\n",
    "        super(features_pyramid, self).__init__()\n",
    "\n",
    "        pool_size = 2\n",
    "        self.avg_pool = nn.AvgPool2d(pool_size)\n",
    "        \n",
    "        n_layers = n_layers\n",
    "        n_filters = 512\n",
    "\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        for i in range(n_layers - 1):\n",
    "            postfix = \"_layer\" + str(i+2)\n",
    "            layer = conv_layer(in_channels,\n",
    "                              filters=n_filters,\n",
    "                              kernel_size=3,\n",
    "                              strides=2,\n",
    "                              use_maxpool=False,\n",
    "                              postfix=postfix)\n",
    "            self.conv_layers.append(layer)\n",
    "            in_channels = n_filters\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = [x]\n",
    "        conv = self.avg_pool(x)\n",
    "        outputs.append(conv)\n",
    "        prev_conv = conv\n",
    "\n",
    "        # additional feature map layers\n",
    "        for convlayer in self.conv_layers:\n",
    "            conv = convlayer(prev_conv)\n",
    "            outputs.append(conv)\n",
    "            prev_conv = conv            \n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1LzRHlpNfVM"
   },
   "source": [
    "### `resnet_v2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NFTvmHr9NeLD"
   },
   "outputs": [],
   "source": [
    "# backbone\n",
    "class resnet_v2(nn.Module):\n",
    "    \"\"\"\n",
    "    # Arguments\n",
    "        input_shape (tensor): Shape of the input image tensor, assumed to be\n",
    "                              (N, C, H, W) following PyTorch tensor convention\n",
    "        depth (int): Number of convolutional layers\n",
    "        num_classes (int): Number of classes\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, depth, n_layers=4):\n",
    "        super(resnet_v2, self).__init__()\n",
    "\n",
    "        # name = 'ResNet%dv2' % (depth)\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        # copied\n",
    "        if (depth - 2) % 9 != 0:\n",
    "            raise ValueError('depth should be 9n+2')\n",
    "\n",
    "        num_filters_in = 16\n",
    "        self.resnet1 = resnet_layer(input_shape[1],\n",
    "                                    num_filters=num_filters_in,\n",
    "                                    conv_first=True)\n",
    "        \n",
    "        self.resnet_blocks = nn.ModuleList()\n",
    "        self.num_res_blocks = (depth - 2) // 9\n",
    "        in_channels = num_filters_in\n",
    "        for stage in range(3):\n",
    "            initial_in_channels = in_channels\n",
    "            for res_block in range(self.num_res_blocks):\n",
    "                activation = 'relu'\n",
    "                batch_normalization = True\n",
    "                strides = 1\n",
    "                if stage == 0:\n",
    "                    num_filters_out = num_filters_in * 4\n",
    "                    if res_block == 0:  # first layer and first stage\n",
    "                        activation = None\n",
    "                        batch_normalization = False\n",
    "                else:\n",
    "                    num_filters_out = num_filters_in * 2\n",
    "                    if res_block == 0:  # first layer but not first stage\n",
    "                        strides = 2 # downsample\n",
    "                \n",
    "                # bottleneck residual unit\n",
    "                self.resnet_blocks.append(\n",
    "                    resnet_layer(in_channels,\n",
    "                                 num_filters=num_filters_in,\n",
    "                                 kernel_size=1,\n",
    "                                 strides=strides,\n",
    "                                 activation=activation,\n",
    "                                 batch_normalization=batch_normalization,\n",
    "                                 conv_first=False))\n",
    "                in_channels = num_filters_in\n",
    "\n",
    "                self.resnet_blocks.append(\n",
    "                    resnet_layer(in_channels,\n",
    "                                 num_filters=num_filters_in,\n",
    "                                 conv_first=False))\n",
    "                in_channels = num_filters_in\n",
    "\n",
    "                self.resnet_blocks.append(\n",
    "                    resnet_layer(in_channels,\n",
    "                                 num_filters=num_filters_out,\n",
    "                                 kernel_size=1,\n",
    "                                 conv_first=False))\n",
    "                in_channels = num_filters_out\n",
    "\n",
    "                if res_block == 0:\n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    self.resnet_blocks.append(\n",
    "                        resnet_layer(initial_in_channels,\n",
    "                                     num_filters=num_filters_out,\n",
    "                                     kernel_size=1,\n",
    "                                     strides=strides,\n",
    "                                     activation=None,\n",
    "                                     batch_normalization=False))\n",
    "                    in_channels = num_filters_out\n",
    "            \n",
    "            num_filters_in = num_filters_out    # don't use self.num_filters_in\n",
    "                                                # since it persists between calls\n",
    "                    # num_filters_out = initial_num_filters_in * 2 * 2 * 2\n",
    "                    #                 = initial_num_filters_in * 8\n",
    "                    #                 = 16 * 8 = 128\n",
    "        \n",
    "        self.batchnorm = nn.BatchNorm2d(num_filters_out)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        self.features_pyramid = features_pyramid(num_filters_out,\n",
    "                                                 n_layers)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet1(x)\n",
    "        # Instantiate the stack of residual units\n",
    "        i = 0\n",
    "        for stage in range(3):\n",
    "            for res_block in range(self.num_res_blocks):\n",
    "                y = self.resnet_blocks[i](x)\n",
    "                i += 1\n",
    "                y = self.resnet_blocks[i](y)\n",
    "                i += 1\n",
    "                y = self.resnet_blocks[i](y)\n",
    "                i += 1\n",
    "                if res_block == 0:\n",
    "                    # linear projection residual shortcut connection to match\n",
    "                    # changed dims\n",
    "                    x = self.resnet_blocks[i](x)\n",
    "                    i += 1\n",
    "                x = x + y\n",
    "\n",
    "        # v2 has BN-ReLU before Pooling\n",
    "        x = self.batchnorm(x)\n",
    "        x = self.activation(x)\n",
    "        # 1st feature map layer\n",
    "\n",
    "        # main feature maps (160, 120)\n",
    "        # succeeding feature maps scaled down by\n",
    "        # 2, 4, 8\n",
    "        outputs = self.features_pyramid(x)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W32nHep-oNCf"
   },
   "source": [
    "### `build_resnet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kIcf4RUGoMBU"
   },
   "outputs": [],
   "source": [
    "def build_resnet(input_shape=(480, 640, 3),\n",
    "                 n_layers=4,\n",
    "                 n=6):\n",
    "    depth = n * 9 + 2\n",
    "    \n",
    "    return resnet_v2(input_shape,\n",
    "                     depth=depth,\n",
    "                     n_layers=n_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TwWaSPYtz4tJ"
   },
   "source": [
    "### `build_fcn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3mepben3z7aN"
   },
   "outputs": [],
   "source": [
    "# analog of `build_fcn`\n",
    "class fcn(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_shape,\n",
    "                 backbone,\n",
    "                 n_layers=4,\n",
    "                 n_classes=4):\n",
    "        super(fcn, self).__init__()\n",
    "\n",
    "        self.backbone = backbone\n",
    "\n",
    "        size = (input_shape[-2] // 4, input_shape[-1] // 4)\n",
    "        feature_size = 8\n",
    "        scale_factor = 2\n",
    "        filters = 256\n",
    "        self.upsamplers = nn.ModuleList()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        in_channels = 16 * 16\n",
    "        total_channels = in_channels\n",
    "        for _ in range(n_layers):\n",
    "            postfix = \"fcn_\" + str(feature_size)\n",
    "            self.conv_layers.append(\n",
    "                conv_layer(in_channels,\n",
    "                           filters=filters,\n",
    "                           use_maxpool=False,\n",
    "                           postfix=postfix)\n",
    "            )\n",
    "            in_channels = 512\n",
    "            total_channels += filters\n",
    "            \n",
    "            postfix = postfix + \"_up2d\"\n",
    "            # self.upsamplers.append(\n",
    "            #     nn.Upsample(scale_factor=scale_factor,\n",
    "            #                           mode='bilinear')\n",
    "            # )\n",
    "            self.upsamplers.append(\n",
    "                nn.Upsample(size=size,\n",
    "                            mode='bilinear')\n",
    "            )\n",
    "            \n",
    "            # scale_factor *= 2\n",
    "            feature_size *= 2\n",
    "        \n",
    "        in_channels = total_channels\n",
    "        filters = 256\n",
    "        self.tconv_layer1 = tconv_layer(in_channels,\n",
    "                                        filters=filters,\n",
    "                                        postfix=\"up_x2\")\n",
    "        \n",
    "        in_channels = filters\n",
    "        self.tconv_layer2 = tconv_layer(in_channels,\n",
    "                                        filters=filters,\n",
    "                                        postfix=\"up_x4\")\n",
    "        \n",
    "        in_channels = filters\n",
    "        kernel_size = 1\n",
    "        padding = kernel_size // 2      # 'same' padding\n",
    "        self.conv_transpose = nn.ConvTranspose2d(in_channels,\n",
    "                                                 n_classes,\n",
    "                                                 kernel_size,\n",
    "                                                 stride=1,\n",
    "                                                 padding=padding)\n",
    "        # 'he_normal' initialization\n",
    "        nn.init.kaiming_normal_(self.conv_transpose.weight)\n",
    "\n",
    "        self.logsoftmax = nn.LogSoftmax()\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "\n",
    "        main_feature = features[0]\n",
    "        features = features[1:]\n",
    "        out_features = [main_feature]\n",
    "\n",
    "        # other half of the features pyramid\n",
    "        # including upsampling to restore the\n",
    "        # feature maps to the dimensions\n",
    "        # equal to 1/4 the image size\n",
    "        for i, feature in enumerate(features):\n",
    "            feature = self.conv_layers[i](feature)\n",
    "            feature = self.upsamplers[i](feature)\n",
    "            out_features.append(feature)\n",
    "\n",
    "        # concatenate all upsampled features\n",
    "        x = torch.cat(out_features, dim=1)          # merge at channel dimension\n",
    "        # perform 2 additional feature extraction\n",
    "        # and upsampling\n",
    "        x = self.tconv_layer1(x)\n",
    "        x = self.tconv_layer2(x)\n",
    "\n",
    "        # generate the pixel-wise classifier\n",
    "        x = self.conv_transpose(x)\n",
    "        x = self.logsoftmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_opjzjpXzW-"
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IlXonTJZNRtF",
    "outputId": "588bd479-4fc8-4439-ce7f-6834f9818bdc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drinks\tdrive  sample_data\n"
     ]
    }
   ],
   "source": [
    "# download dataset\n",
    "###!mkdir 'drive/My Drive/coe197f/dataset'\n",
    "###!tar -xf 'drive/My Drive/coe197f/drinks.tar.gz' -C 'drive/My Drive/coe197f/dataset'\n",
    "\n",
    "# !tar -xf 'drive/My Drive/coe197f/drinks.tar.gz'\n",
    "# !ls\n",
    "\n",
    "# !cp -r 'drive/My Drive/coe197f/dataset/drinks' .\n",
    "# !ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_9QFpRtgMglC"
   },
   "source": [
    "### Define dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XmGrsmwiYByi"
   },
   "outputs": [],
   "source": [
    "class SemanticSegmentationDataset(Dataset):\n",
    "    def __init__(self, data_dir, gt_fname, cuda=True):\n",
    "        self.data_dir = data_dir\n",
    "        gt_path = os.path.join(data_dir, gt_fname)\n",
    "\n",
    "        self.gt_dict = np.load(gt_path,\n",
    "                               allow_pickle=True).flat[0]\n",
    "        self.img_names = list(self.gt_dict.keys())\n",
    "\n",
    "        self.cuda = cuda\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.data_dir, img_name)\n",
    "        img = PIL.Image.open(img_path)\n",
    "        img = transforms.ToTensor()(img)    # C, H, W\n",
    "\n",
    "        gt = self.gt_dict[img_name].transpose(2,0,1)    # C, H, W\n",
    "        gt = torch.from_numpy(gt).long()\n",
    "\n",
    "        if cuda:\n",
    "            img = img.cuda()\n",
    "            gt = gt.cuda()\n",
    "\n",
    "        return img, gt  # torch (cuda) CHW\n",
    "    \n",
    "    def get_img_names(self, sorted=True):\n",
    "        \"\"\"Returns the list of image files in `self.data_dir`,\n",
    "            but only in that directory (no recursive search)\n",
    "        \"\"\"\n",
    "        img_exts = {'.jpg', '.jpeg', '.png', '.bmp'}\n",
    "\n",
    "        img_names = []\n",
    "        _, _, files = next(os.walk(self.img_names))\n",
    "        for fname in sorted(files):\n",
    "            if os.path.splitext(fname)[1].lower() in img_exts:\n",
    "                img_names.append(fname)\n",
    "        \n",
    "        return img_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DvxmzCibNHAC"
   },
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aDW79nBZM74b"
   },
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "shuffle = True\n",
    "epochs = 100\n",
    "T_save = 10\n",
    "T_print = 200\n",
    "num_classes = 4\n",
    "cuda = True\n",
    "\n",
    "pretrained_weight_fname = None\n",
    "weights_dir = 'weights'\n",
    "data_dir = 'drinks/'\n",
    "train_gt_fname = 'segmentation_train.npy'\n",
    "test_gt_fname = 'segmentation_test.npy'\n",
    "Path(weights_dir).mkdir(parents=True, exist_ok=True)    # make path if not exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TEizBXs-MbNh"
   },
   "source": [
    "### Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qa8psUDXrTDG"
   },
   "outputs": [],
   "source": [
    "trainset = SemanticSegmentationDataset(data_dir, train_gt_fname, cuda=cuda)\n",
    "trainloader = DataLoader(trainset,\n",
    "                         batch_size=batch_size,\n",
    "                         shuffle=shuffle)\n",
    "# testset = SemanticSegmentationDataset(data_dir, test_gt_fname, cuda=cuda)\n",
    "# testloader = DataLoader(testset,\n",
    "#                         batch_size=batch_size,\n",
    "#                         shuffle=shuffle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A-Fe58MV32Oc"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oblePHiXIV9W"
   },
   "source": [
    "### Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "chw0aC5WGKJU"
   },
   "outputs": [],
   "source": [
    "channels, height, width = 3, 480, 640\n",
    "input_shape = (batch_size, channels, height, width)\n",
    "\n",
    "if cuda:\n",
    "    backbone = build_resnet(input_shape=input_shape).cuda()\n",
    "    model = fcn(input_shape, backbone).cuda()\n",
    "else:\n",
    "    backbone = build_resnet(input_shape=input_shape)\n",
    "    model = fcn(input_shape, backbone)\n",
    "\n",
    "\n",
    "if pretrained_weight_fname:\n",
    "    restore_weights(model, weight_dir, pretrained_weight_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d80KZvYkIZwt"
   },
   "source": [
    "### Functions needed for train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27GOsDmgDrgg"
   },
   "outputs": [],
   "source": [
    "# learning rate scheduler\n",
    "def lr_scheduler(epoch):\n",
    "    lr = 1e-3\n",
    "    if epoch > 80:\n",
    "        lr *= 5e-2\n",
    "    elif epoch > 60:\n",
    "        lr *= 1e-1\n",
    "    elif epoch > 40:\n",
    "        lr *= 5e-1\n",
    "    \n",
    "    print('Learning rate:', lr)\n",
    "\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dP5XLheAFmyb"
   },
   "outputs": [],
   "source": [
    "def save_model(model, weights_dir, weight_fname):\n",
    "    weight_path = os.path.join(weights_dir, weight_fname)\n",
    "    print('Saving weights to', weight_path)\n",
    "    torch.save(model.state_dict(), weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WUl0DcFhIgaR"
   },
   "outputs": [],
   "source": [
    "def restore_weights(model, weights_dir, weight_fname):\n",
    "    weight_path = os.path.join(weights_dir, weight_fname)\n",
    "    print('Restoring weights from', weight_path)\n",
    "    model.load_state_dict(torch.load(weight_path))\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DiEiOx_1IeQO"
   },
   "source": [
    "### Train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 437
    },
    "colab_type": "code",
    "id": "a7qNVWEp34kq",
    "outputId": "136febc3-d551-4114-b4a8-094ad2d66718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eee198/anaconda3/envs/ocr2/lib/python3.7/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n",
      "/home/eee198/anaconda3/envs/ocr2/lib/python3.7/site-packages/ipykernel_launcher.py:94: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200-th minibatch\tloss: 4.581914\n",
      "85.96934747695923 secs elapsed\n",
      "400-th minibatch\tloss: 8.212995\n",
      "171.13247966766357 secs elapsed\n",
      "600-th minibatch\tloss: 11.014925\n",
      "256.33518290519714 secs elapsed\n",
      "800-th minibatch\tloss: 13.149916\n",
      "341.4132831096649 secs elapsed\n",
      "1000-th minibatch\tloss: 14.704585\n",
      "426.43683218955994 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   1\tloss: 2940.917053\n",
      "426.4369378089905 secs elapsed\n",
      "Saving weights to weights/resnet_v2-1epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 1.198146\n",
      "510.89235186576843 secs elapsed\n",
      "400-th minibatch\tloss: 2.177213\n",
      "595.6450202465057 secs elapsed\n",
      "600-th minibatch\tloss: 2.979715\n",
      "679.6575660705566 secs elapsed\n",
      "800-th minibatch\tloss: 3.721677\n",
      "763.6524167060852 secs elapsed\n",
      "1000-th minibatch\tloss: 4.373766\n",
      "848.0936834812164 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   2\tloss: 874.753138\n",
      "848.0937876701355 secs elapsed\n",
      "Saving weights to weights/resnet_v2-2epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.616056\n",
      "932.5380051136017 secs elapsed\n",
      "400-th minibatch\tloss: 1.221732\n",
      "1016.9374160766602 secs elapsed\n",
      "600-th minibatch\tloss: 1.814712\n",
      "1101.3693704605103 secs elapsed\n",
      "800-th minibatch\tloss: 2.360364\n",
      "1185.8316233158112 secs elapsed\n",
      "1000-th minibatch\tloss: 2.894772\n",
      "1270.1925508975983 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   3\tloss: 578.954480\n",
      "1270.1926550865173 secs elapsed\n",
      "Saving weights to weights/resnet_v2-3epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.522795\n",
      "1354.6791653633118 secs elapsed\n",
      "400-th minibatch\tloss: 1.029342\n",
      "1439.0961155891418 secs elapsed\n",
      "600-th minibatch\tloss: 1.507762\n",
      "1523.4082362651825 secs elapsed\n",
      "800-th minibatch\tloss: 2.010369\n",
      "1607.829467535019 secs elapsed\n",
      "1000-th minibatch\tloss: 2.480742\n",
      "1692.2596361637115 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   4\tloss: 496.148480\n",
      "1692.2597360610962 secs elapsed\n",
      "Saving weights to weights/resnet_v2-4epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.480479\n",
      "1776.6903884410858 secs elapsed\n",
      "400-th minibatch\tloss: 0.920713\n",
      "1861.064531326294 secs elapsed\n",
      "600-th minibatch\tloss: 1.357178\n",
      "1945.4438080787659 secs elapsed\n",
      "800-th minibatch\tloss: 1.780694\n",
      "2029.8323719501495 secs elapsed\n",
      "1000-th minibatch\tloss: 2.188484\n",
      "2114.273907661438 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   5\tloss: 437.696759\n",
      "2114.27406334877 secs elapsed\n",
      "Saving weights to weights/resnet_v2-5epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.408213\n",
      "2198.751799583435 secs elapsed\n",
      "400-th minibatch\tloss: 0.785559\n",
      "2283.1249282360077 secs elapsed\n",
      "600-th minibatch\tloss: 1.178845\n",
      "2367.57537984848 secs elapsed\n",
      "800-th minibatch\tloss: 1.567729\n",
      "2452.0361683368683 secs elapsed\n",
      "1000-th minibatch\tloss: 1.941113\n",
      "2536.4123668670654 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   6\tloss: 388.222634\n",
      "2536.4125168323517 secs elapsed\n",
      "Saving weights to weights/resnet_v2-6epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.363353\n",
      "2620.861545085907 secs elapsed\n",
      "400-th minibatch\tloss: 0.728684\n",
      "2705.2752866744995 secs elapsed\n",
      "600-th minibatch\tloss: 1.067965\n",
      "2789.5939888954163 secs elapsed\n",
      "800-th minibatch\tloss: 1.407230\n",
      "2873.986060857773 secs elapsed\n",
      "1000-th minibatch\tloss: 1.735007\n",
      "2958.3199026584625 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   7\tloss: 347.001374\n",
      "2958.32000541687 secs elapsed\n",
      "Saving weights to weights/resnet_v2-7epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.313563\n",
      "3042.7464456558228 secs elapsed\n",
      "400-th minibatch\tloss: 0.649490\n",
      "3127.124186515808 secs elapsed\n",
      "600-th minibatch\tloss: 0.982641\n",
      "3211.4797377586365 secs elapsed\n",
      "800-th minibatch\tloss: 1.272277\n",
      "3295.8963344097137 secs elapsed\n",
      "1000-th minibatch\tloss: 1.567863\n",
      "3380.3270025253296 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   8\tloss: 313.572529\n",
      "3380.3271045684814 secs elapsed\n",
      "Saving weights to weights/resnet_v2-8epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.301634\n",
      "3464.747813940048 secs elapsed\n",
      "400-th minibatch\tloss: 0.598382\n",
      "3549.1386229991913 secs elapsed\n",
      "600-th minibatch\tloss: 0.882784\n",
      "3633.5380907058716 secs elapsed\n",
      "800-th minibatch\tloss: 1.175294\n",
      "3717.8835911750793 secs elapsed\n",
      "1000-th minibatch\tloss: 1.434689\n",
      "3801.852669239044 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch   9\tloss: 286.937767\n",
      "3801.8530950546265 secs elapsed\n",
      "Saving weights to weights/resnet_v2-9epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.278190\n",
      "3886.24626994133 secs elapsed\n",
      "400-th minibatch\tloss: 0.527582\n",
      "3970.6151518821716 secs elapsed\n",
      "600-th minibatch\tloss: 0.797289\n",
      "4055.0141372680664 secs elapsed\n",
      "800-th minibatch\tloss: 1.065992\n",
      "4139.388953924179 secs elapsed\n",
      "1000-th minibatch\tloss: 1.328012\n",
      "4223.73176407814 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  10\tloss: 265.602457\n",
      "4223.731867551804 secs elapsed\n",
      "Saving weights to weights/resnet_v2-10epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.246281\n",
      "4308.122858285904 secs elapsed\n",
      "400-th minibatch\tloss: 0.500290\n",
      "4392.484087705612 secs elapsed\n",
      "600-th minibatch\tloss: 0.749506\n",
      "4476.875591516495 secs elapsed\n",
      "800-th minibatch\tloss: 1.010221\n",
      "4561.229089975357 secs elapsed\n",
      "1000-th minibatch\tloss: 1.239711\n",
      "4645.600075483322 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  11\tloss: 247.942284\n",
      "4645.600175380707 secs elapsed\n",
      "Saving weights to weights/resnet_v2-11epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.242420\n",
      "4730.095515489578 secs elapsed\n",
      "400-th minibatch\tloss: 0.458263\n",
      "4814.509176015854 secs elapsed\n",
      "600-th minibatch\tloss: 0.689031\n",
      "4898.874838113785 secs elapsed\n",
      "800-th minibatch\tloss: 0.927842\n",
      "4983.232201337814 secs elapsed\n",
      "1000-th minibatch\tloss: 1.165237\n",
      "5067.632031202316 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  12\tloss: 233.047434\n",
      "5067.632134437561 secs elapsed\n",
      "Saving weights to weights/resnet_v2-12epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.224531\n",
      "5152.090698480606 secs elapsed\n",
      "400-th minibatch\tloss: 0.439639\n",
      "5236.443008899689 secs elapsed\n",
      "600-th minibatch\tloss: 0.676356\n",
      "5320.914047241211 secs elapsed\n",
      "800-th minibatch\tloss: 0.890564\n",
      "5405.338714599609 secs elapsed\n",
      "1000-th minibatch\tloss: 1.100669\n",
      "5489.687948226929 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  13\tloss: 220.133886\n",
      "5489.688091754913 secs elapsed\n",
      "Saving weights to weights/resnet_v2-13epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.218219\n",
      "5574.103328704834 secs elapsed\n",
      "400-th minibatch\tloss: 0.431191\n",
      "5658.5020253658295 secs elapsed\n",
      "600-th minibatch\tloss: 0.629965\n",
      "5742.954900026321 secs elapsed\n",
      "800-th minibatch\tloss: 0.832928\n",
      "5827.381442308426 secs elapsed\n",
      "1000-th minibatch\tloss: 1.043222\n",
      "5911.811932563782 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  14\tloss: 208.644306\n",
      "5911.812288761139 secs elapsed\n",
      "Saving weights to weights/resnet_v2-14epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.204870\n",
      "5996.186181783676 secs elapsed\n",
      "400-th minibatch\tloss: 0.409009\n",
      "6080.390029430389 secs elapsed\n",
      "600-th minibatch\tloss: 0.598298\n",
      "6163.914962768555 secs elapsed\n",
      "800-th minibatch\tloss: 0.800410\n",
      "6248.200560092926 secs elapsed\n",
      "1000-th minibatch\tloss: 0.992026\n",
      "6332.645668029785 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  15\tloss: 198.405246\n",
      "6332.645779132843 secs elapsed\n",
      "Saving weights to weights/resnet_v2-15epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.190725\n",
      "6417.051222324371 secs elapsed\n",
      "400-th minibatch\tloss: 0.378617\n",
      "6501.475292682648 secs elapsed\n",
      "600-th minibatch\tloss: 0.568302\n",
      "6585.849165201187 secs elapsed\n",
      "800-th minibatch\tloss: 0.759319\n",
      "6670.140551805496 secs elapsed\n",
      "1000-th minibatch\tloss: 0.944567\n",
      "6754.5131022930145 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  16\tloss: 188.913430\n",
      "6754.513598680496 secs elapsed\n",
      "Saving weights to weights/resnet_v2-16epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.187272\n",
      "6838.9153344631195 secs elapsed\n",
      "400-th minibatch\tloss: 0.376170\n",
      "6923.254191875458 secs elapsed\n",
      "600-th minibatch\tloss: 0.552851\n",
      "7007.651355743408 secs elapsed\n",
      "800-th minibatch\tloss: 0.730008\n",
      "7091.74601149559 secs elapsed\n",
      "1000-th minibatch\tloss: 0.900749\n",
      "7175.624544620514 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  17\tloss: 180.149886\n",
      "7175.624699831009 secs elapsed\n",
      "Saving weights to weights/resnet_v2-17epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.180569\n",
      "7259.682308912277 secs elapsed\n",
      "400-th minibatch\tloss: 0.347576\n",
      "7343.884154558182 secs elapsed\n",
      "600-th minibatch\tloss: 0.523017\n",
      "7428.264575719833 secs elapsed\n",
      "800-th minibatch\tloss: 0.687280\n",
      "7512.66827750206 secs elapsed\n",
      "1000-th minibatch\tloss: 0.860194\n",
      "7597.095091342926 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  18\tloss: 172.038858\n",
      "7597.095194816589 secs elapsed\n",
      "Saving weights to weights/resnet_v2-18epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.175474\n",
      "7681.539307832718 secs elapsed\n",
      "400-th minibatch\tloss: 0.321819\n",
      "7765.950911283493 secs elapsed\n",
      "600-th minibatch\tloss: 0.485160\n",
      "7850.375803232193 secs elapsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800-th minibatch\tloss: 0.651412\n",
      "7934.737657546997 secs elapsed\n",
      "1000-th minibatch\tloss: 0.822142\n",
      "8019.1044244766235 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  19\tloss: 164.428481\n",
      "8019.104849815369 secs elapsed\n",
      "Saving weights to weights/resnet_v2-19epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.146913\n",
      "8103.510803222656 secs elapsed\n",
      "400-th minibatch\tloss: 0.307431\n",
      "8187.867605686188 secs elapsed\n",
      "600-th minibatch\tloss: 0.465821\n",
      "8272.181189775467 secs elapsed\n",
      "800-th minibatch\tloss: 0.621843\n",
      "8356.51589512825 secs elapsed\n",
      "1000-th minibatch\tloss: 0.785738\n",
      "8440.878652095795 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  20\tloss: 157.147622\n",
      "8440.878759860992 secs elapsed\n",
      "Saving weights to weights/resnet_v2-20epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.151436\n",
      "8525.328181505203 secs elapsed\n",
      "400-th minibatch\tloss: 0.298677\n",
      "8609.737678050995 secs elapsed\n",
      "600-th minibatch\tloss: 0.449831\n",
      "8694.12981390953 secs elapsed\n",
      "800-th minibatch\tloss: 0.591007\n",
      "8778.481040716171 secs elapsed\n",
      "1000-th minibatch\tloss: 0.752329\n",
      "8862.850555181503 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  21\tloss: 150.465890\n",
      "8862.850663661957 secs elapsed\n",
      "Saving weights to weights/resnet_v2-21epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.145400\n",
      "8947.273034334183 secs elapsed\n",
      "400-th minibatch\tloss: 0.296013\n",
      "9031.685157299042 secs elapsed\n",
      "600-th minibatch\tloss: 0.442967\n",
      "9116.092402935028 secs elapsed\n",
      "800-th minibatch\tloss: 0.578972\n",
      "9200.499644517899 secs elapsed\n",
      "1000-th minibatch\tloss: 0.720539\n",
      "9284.839456319809 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  22\tloss: 144.107891\n",
      "9284.839609146118 secs elapsed\n",
      "Saving weights to weights/resnet_v2-22epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.143358\n",
      "9369.32312297821 secs elapsed\n",
      "400-th minibatch\tloss: 0.289109\n",
      "9453.772576332092 secs elapsed\n",
      "600-th minibatch\tloss: 0.426442\n",
      "9538.153894424438 secs elapsed\n",
      "800-th minibatch\tloss: 0.556757\n",
      "9622.571955680847 secs elapsed\n",
      "1000-th minibatch\tloss: 0.690781\n",
      "9706.935092926025 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  23\tloss: 138.156143\n",
      "9706.935193300247 secs elapsed\n",
      "Saving weights to weights/resnet_v2-23epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.135888\n",
      "9791.383231639862 secs elapsed\n",
      "400-th minibatch\tloss: 0.268380\n",
      "9875.781075954437 secs elapsed\n",
      "600-th minibatch\tloss: 0.400794\n",
      "9960.201806545258 secs elapsed\n",
      "800-th minibatch\tloss: 0.538956\n",
      "10044.580318689346 secs elapsed\n",
      "1000-th minibatch\tloss: 0.663216\n",
      "10128.969026327133 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  24\tloss: 132.643123\n",
      "10128.969126939774 secs elapsed\n",
      "Saving weights to weights/resnet_v2-24epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.133245\n",
      "10213.401230573654 secs elapsed\n",
      "400-th minibatch\tloss: 0.259005\n",
      "10297.807695388794 secs elapsed\n",
      "600-th minibatch\tloss: 0.389163\n",
      "10382.19653081894 secs elapsed\n",
      "800-th minibatch\tloss: 0.518543\n",
      "10466.591731786728 secs elapsed\n",
      "1000-th minibatch\tloss: 0.636248\n",
      "10551.062339305878 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  25\tloss: 127.249633\n",
      "10551.062440156937 secs elapsed\n",
      "Saving weights to weights/resnet_v2-25epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.129508\n",
      "10635.545959949493 secs elapsed\n",
      "400-th minibatch\tloss: 0.244447\n",
      "10719.908596754074 secs elapsed\n",
      "600-th minibatch\tloss: 0.370652\n",
      "10804.234674215317 secs elapsed\n",
      "800-th minibatch\tloss: 0.489566\n",
      "10888.618818759918 secs elapsed\n",
      "1000-th minibatch\tloss: 0.611446\n",
      "10972.981975078583 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  26\tloss: 122.289116\n",
      "10972.982075929642 secs elapsed\n",
      "Saving weights to weights/resnet_v2-26epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.118585\n",
      "11057.384531259537 secs elapsed\n",
      "400-th minibatch\tloss: 0.242109\n",
      "11141.760029554367 secs elapsed\n",
      "600-th minibatch\tloss: 0.360664\n",
      "11226.09396147728 secs elapsed\n",
      "800-th minibatch\tloss: 0.476080\n",
      "11310.482172489166 secs elapsed\n",
      "1000-th minibatch\tloss: 0.587476\n",
      "11394.844903707504 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  27\tloss: 117.495165\n",
      "11394.84500670433 secs elapsed\n",
      "Saving weights to weights/resnet_v2-27epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.115497\n",
      "11479.268549203873 secs elapsed\n",
      "400-th minibatch\tloss: 0.235378\n",
      "11563.690019607544 secs elapsed\n",
      "600-th minibatch\tloss: 0.349135\n",
      "11648.046444654465 secs elapsed\n",
      "800-th minibatch\tloss: 0.457609\n",
      "11732.385818004608 secs elapsed\n",
      "1000-th minibatch\tloss: 0.564861\n",
      "11816.751699924469 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  28\tloss: 112.972119\n",
      "11816.751801013947 secs elapsed\n",
      "Saving weights to weights/resnet_v2-28epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.111580\n",
      "11901.137551546097 secs elapsed\n",
      "400-th minibatch\tloss: 0.219707\n",
      "11985.510488986969 secs elapsed\n",
      "600-th minibatch\tloss: 0.324484\n",
      "12069.89117860794 secs elapsed\n",
      "800-th minibatch\tloss: 0.428441\n",
      "12154.341651916504 secs elapsed\n",
      "1000-th minibatch\tloss: 0.543618\n",
      "12238.745359182358 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  29\tloss: 108.723666\n",
      "12238.745462656021 secs elapsed\n",
      "Saving weights to weights/resnet_v2-29epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.104875\n",
      "12323.153933048248 secs elapsed\n",
      "400-th minibatch\tloss: 0.210177\n",
      "12407.535103321075 secs elapsed\n",
      "600-th minibatch\tloss: 0.318033\n",
      "12491.835499286652 secs elapsed\n",
      "800-th minibatch\tloss: 0.426504\n",
      "12576.18241071701 secs elapsed\n",
      "1000-th minibatch\tloss: 0.522956\n",
      "12660.558543205261 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  30\tloss: 104.591196\n",
      "12660.558648109436 secs elapsed\n",
      "Saving weights to weights/resnet_v2-30epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.105165\n",
      "12744.965929985046 secs elapsed\n",
      "400-th minibatch\tloss: 0.201473\n",
      "12829.370804548264 secs elapsed\n",
      "600-th minibatch\tloss: 0.305307\n",
      "12913.037615537643 secs elapsed\n",
      "800-th minibatch\tloss: 0.405572\n",
      "12996.507618665695 secs elapsed\n",
      "1000-th minibatch\tloss: 0.503562\n",
      "13080.763768672943 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  31\tloss: 100.712419\n",
      "13080.763877391815 secs elapsed\n",
      "Saving weights to weights/resnet_v2-31epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.102168\n",
      "13165.182603597641 secs elapsed\n",
      "400-th minibatch\tloss: 0.196644\n",
      "13249.538292884827 secs elapsed\n",
      "600-th minibatch\tloss: 0.295038\n",
      "13333.891840457916 secs elapsed\n",
      "800-th minibatch\tloss: 0.391655\n",
      "13418.256437540054 secs elapsed\n",
      "1000-th minibatch\tloss: 0.485255\n",
      "13502.590344667435 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  32\tloss: 97.050949\n",
      "13502.590446472168 secs elapsed\n",
      "Saving weights to weights/resnet_v2-32epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.087788\n",
      "13586.949661493301 secs elapsed\n",
      "400-th minibatch\tloss: 0.183021\n",
      "13671.348722696304 secs elapsed\n",
      "600-th minibatch\tloss: 0.280656\n",
      "13755.753220558167 secs elapsed\n",
      "800-th minibatch\tloss: 0.369259\n",
      "13840.157025575638 secs elapsed\n",
      "1000-th minibatch\tloss: 0.467099\n",
      "13924.54536986351 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  33\tloss: 93.419766\n",
      "13924.54547214508 secs elapsed\n",
      "Saving weights to weights/resnet_v2-33epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.090429\n",
      "14008.979236841202 secs elapsed\n",
      "400-th minibatch\tloss: 0.179752\n",
      "14093.376411914825 secs elapsed\n",
      "600-th minibatch\tloss: 0.269142\n",
      "14177.751991271973 secs elapsed\n",
      "800-th minibatch\tloss: 0.359609\n",
      "14262.16780948639 secs elapsed\n",
      "1000-th minibatch\tloss: 0.450227\n",
      "14346.588562250137 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  34\tloss: 90.045383\n",
      "14346.58867073059 secs elapsed\n",
      "Saving weights to weights/resnet_v2-34epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.088575\n",
      "14431.09912443161 secs elapsed\n",
      "400-th minibatch\tloss: 0.175290\n",
      "14515.508682966232 secs elapsed\n",
      "600-th minibatch\tloss: 0.259631\n",
      "14599.909712314606 secs elapsed\n",
      "800-th minibatch\tloss: 0.351246\n",
      "14684.302634954453 secs elapsed\n",
      "1000-th minibatch\tloss: 0.433942\n",
      "14768.668600082397 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  35\tloss: 86.788339\n",
      "14768.66870522499 secs elapsed\n",
      "Saving weights to weights/resnet_v2-35epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.085649\n",
      "14853.076031684875 secs elapsed\n",
      "400-th minibatch\tloss: 0.170056\n",
      "14937.46251130104 secs elapsed\n",
      "600-th minibatch\tloss: 0.252829\n",
      "15021.84328007698 secs elapsed\n",
      "800-th minibatch\tloss: 0.332095\n",
      "15106.214872837067 secs elapsed\n",
      "1000-th minibatch\tloss: 0.418574\n",
      "15190.635535240173 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  36\tloss: 83.714845\n",
      "15190.635638475418 secs elapsed\n",
      "Saving weights to weights/resnet_v2-36epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.085135\n",
      "15275.035703420639 secs elapsed\n",
      "400-th minibatch\tloss: 0.162364\n",
      "15359.42341709137 secs elapsed\n",
      "600-th minibatch\tloss: 0.239838\n",
      "15443.80227971077 secs elapsed\n",
      "800-th minibatch\tloss: 0.324306\n",
      "15528.144602298737 secs elapsed\n",
      "1000-th minibatch\tloss: 0.404146\n",
      "15612.561157941818 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  37\tloss: 80.829225\n",
      "15612.561311721802 secs elapsed\n",
      "Saving weights to weights/resnet_v2-37epoch.pth\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200-th minibatch\tloss: 0.074677\n",
      "15696.986911773682 secs elapsed\n",
      "400-th minibatch\tloss: 0.154544\n",
      "15781.312609434128 secs elapsed\n",
      "600-th minibatch\tloss: 0.232511\n",
      "15865.730207443237 secs elapsed\n",
      "800-th minibatch\tloss: 0.305579\n",
      "15950.112293481827 secs elapsed\n",
      "1000-th minibatch\tloss: 0.389460\n",
      "16034.49278640747 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  38\tloss: 77.891923\n",
      "16034.492938756943 secs elapsed\n",
      "Saving weights to weights/resnet_v2-38epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.072700\n",
      "16118.942483901978 secs elapsed\n",
      "400-th minibatch\tloss: 0.149137\n",
      "16203.38045668602 secs elapsed\n",
      "600-th minibatch\tloss: 0.224256\n",
      "16287.765153169632 secs elapsed\n",
      "800-th minibatch\tloss: 0.302450\n",
      "16372.22155880928 secs elapsed\n",
      "1000-th minibatch\tloss: 0.376041\n",
      "16456.679702043533 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  39\tloss: 75.208233\n",
      "16456.679806232452 secs elapsed\n",
      "Saving weights to weights/resnet_v2-39epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.071803\n",
      "16541.132654190063 secs elapsed\n",
      "400-th minibatch\tloss: 0.145720\n",
      "16625.448808670044 secs elapsed\n",
      "600-th minibatch\tloss: 0.216989\n",
      "16709.869079828262 secs elapsed\n",
      "800-th minibatch\tloss: 0.291178\n",
      "16794.26404452324 secs elapsed\n",
      "1000-th minibatch\tloss: 0.362885\n",
      "16878.64810037613 secs elapsed\n",
      "Learning rate: 0.001\n",
      "epoch  40\tloss: 72.576985\n",
      "16878.648201704025 secs elapsed\n",
      "Saving weights to weights/resnet_v2-40epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.070464\n",
      "16963.06857609749 secs elapsed\n",
      "400-th minibatch\tloss: 0.140334\n",
      "17047.45273399353 secs elapsed\n",
      "600-th minibatch\tloss: 0.213963\n",
      "17131.788386821747 secs elapsed\n",
      "800-th minibatch\tloss: 0.281247\n",
      "17216.15324163437 secs elapsed\n",
      "1000-th minibatch\tloss: 0.350548\n",
      "17300.523416757584 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  41\tloss: 70.109651\n",
      "17300.523522138596 secs elapsed\n",
      "Saving weights to weights/resnet_v2-41epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.065353\n",
      "17384.943128347397 secs elapsed\n",
      "400-th minibatch\tloss: 0.132986\n",
      "17469.35898399353 secs elapsed\n",
      "600-th minibatch\tloss: 0.200148\n",
      "17553.779567480087 secs elapsed\n",
      "800-th minibatch\tloss: 0.269923\n",
      "17638.19084262848 secs elapsed\n",
      "1000-th minibatch\tloss: 0.338430\n",
      "17722.566653966904 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  42\tloss: 67.686066\n",
      "17722.56680560112 secs elapsed\n",
      "Saving weights to weights/resnet_v2-42epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.064158\n",
      "17807.047839164734 secs elapsed\n",
      "400-th minibatch\tloss: 0.134605\n",
      "17891.431324005127 secs elapsed\n",
      "600-th minibatch\tloss: 0.201058\n",
      "17975.907271385193 secs elapsed\n",
      "800-th minibatch\tloss: 0.266853\n",
      "18060.316190242767 secs elapsed\n",
      "1000-th minibatch\tloss: 0.332570\n",
      "18144.63492155075 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  43\tloss: 66.514014\n",
      "18144.635075330734 secs elapsed\n",
      "Saving weights to weights/resnet_v2-43epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.064411\n",
      "18229.04630589485 secs elapsed\n",
      "400-th minibatch\tloss: 0.126693\n",
      "18313.386343717575 secs elapsed\n",
      "600-th minibatch\tloss: 0.195684\n",
      "18397.693961143494 secs elapsed\n",
      "800-th minibatch\tloss: 0.262365\n",
      "18482.08288526535 secs elapsed\n",
      "1000-th minibatch\tloss: 0.326844\n",
      "18566.414535045624 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  44\tloss: 65.368730\n",
      "18566.414638996124 secs elapsed\n",
      "Saving weights to weights/resnet_v2-44epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.065123\n",
      "18650.810113430023 secs elapsed\n",
      "400-th minibatch\tloss: 0.128186\n",
      "18735.184014081955 secs elapsed\n",
      "600-th minibatch\tloss: 0.193772\n",
      "18819.55330300331 secs elapsed\n",
      "800-th minibatch\tloss: 0.257213\n",
      "18903.95923614502 secs elapsed\n",
      "1000-th minibatch\tloss: 0.321180\n",
      "18988.391648054123 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  45\tloss: 64.235983\n",
      "18988.39175271988 secs elapsed\n",
      "Saving weights to weights/resnet_v2-45epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.066209\n",
      "19072.834718942642 secs elapsed\n",
      "400-th minibatch\tloss: 0.131383\n",
      "19157.233239650726 secs elapsed\n",
      "600-th minibatch\tloss: 0.193792\n",
      "19241.5630133152 secs elapsed\n",
      "800-th minibatch\tloss: 0.255495\n",
      "19325.850443840027 secs elapsed\n",
      "1000-th minibatch\tloss: 0.315382\n",
      "19410.208189487457 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  46\tloss: 63.076465\n",
      "19410.208292245865 secs elapsed\n",
      "Saving weights to weights/resnet_v2-46epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.061937\n",
      "19494.63777923584 secs elapsed\n",
      "400-th minibatch\tloss: 0.125300\n",
      "19579.02440047264 secs elapsed\n",
      "600-th minibatch\tloss: 0.190256\n",
      "19663.455939769745 secs elapsed\n",
      "800-th minibatch\tloss: 0.252645\n",
      "19747.78136396408 secs elapsed\n",
      "1000-th minibatch\tloss: 0.309973\n",
      "19832.204870939255 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  47\tloss: 61.994660\n",
      "19832.20501756668 secs elapsed\n",
      "Saving weights to weights/resnet_v2-47epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.059130\n",
      "19916.644557714462 secs elapsed\n",
      "400-th minibatch\tloss: 0.121366\n",
      "20001.04367876053 secs elapsed\n",
      "600-th minibatch\tloss: 0.187609\n",
      "20085.437458992004 secs elapsed\n",
      "800-th minibatch\tloss: 0.242393\n",
      "20169.915365219116 secs elapsed\n",
      "1000-th minibatch\tloss: 0.304637\n",
      "20254.312308311462 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  48\tloss: 60.927484\n",
      "20254.312410116196 secs elapsed\n",
      "Saving weights to weights/resnet_v2-48epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.059109\n",
      "20338.746779203415 secs elapsed\n",
      "400-th minibatch\tloss: 0.116936\n",
      "20423.110592842102 secs elapsed\n",
      "600-th minibatch\tloss: 0.176943\n",
      "20507.53560781479 secs elapsed\n",
      "800-th minibatch\tloss: 0.236997\n",
      "20591.883790016174 secs elapsed\n",
      "1000-th minibatch\tloss: 0.299531\n",
      "20676.28372645378 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  49\tloss: 59.906205\n",
      "20676.283825159073 secs elapsed\n",
      "Saving weights to weights/resnet_v2-49epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.060860\n",
      "20760.648588895798 secs elapsed\n",
      "400-th minibatch\tloss: 0.118835\n",
      "20845.028246879578 secs elapsed\n",
      "600-th minibatch\tloss: 0.175671\n",
      "20929.373764038086 secs elapsed\n",
      "800-th minibatch\tloss: 0.234369\n",
      "21013.752853870392 secs elapsed\n",
      "1000-th minibatch\tloss: 0.294314\n",
      "21098.124499320984 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  50\tloss: 58.862861\n",
      "21098.12464761734 secs elapsed\n",
      "Saving weights to weights/resnet_v2-50epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.056768\n",
      "21182.546187639236 secs elapsed\n",
      "400-th minibatch\tloss: 0.115020\n",
      "21266.991457223892 secs elapsed\n",
      "600-th minibatch\tloss: 0.176626\n",
      "21351.396847486496 secs elapsed\n",
      "800-th minibatch\tloss: 0.234932\n",
      "21435.838537454605 secs elapsed\n",
      "1000-th minibatch\tloss: 0.289119\n",
      "21520.23754787445 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  51\tloss: 57.823824\n",
      "21520.237838745117 secs elapsed\n",
      "Saving weights to weights/resnet_v2-51epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.059000\n",
      "21604.672427654266 secs elapsed\n",
      "400-th minibatch\tloss: 0.115242\n",
      "21689.03746843338 secs elapsed\n",
      "600-th minibatch\tloss: 0.169304\n",
      "21773.472319602966 secs elapsed\n",
      "800-th minibatch\tloss: 0.226475\n",
      "21857.888236522675 secs elapsed\n",
      "1000-th minibatch\tloss: 0.284257\n",
      "21942.310871601105 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  52\tloss: 56.851359\n",
      "21942.310975074768 secs elapsed\n",
      "Saving weights to weights/resnet_v2-52epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.053871\n",
      "22026.731814861298 secs elapsed\n",
      "400-th minibatch\tloss: 0.109977\n",
      "22111.098240852356 secs elapsed\n",
      "600-th minibatch\tloss: 0.163211\n",
      "22195.465260267258 secs elapsed\n",
      "800-th minibatch\tloss: 0.219720\n",
      "22279.780183553696 secs elapsed\n",
      "1000-th minibatch\tloss: 0.279449\n",
      "22364.138649463654 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  53\tloss: 55.889702\n",
      "22364.138753414154 secs elapsed\n",
      "Saving weights to weights/resnet_v2-53epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.052878\n",
      "22448.560967445374 secs elapsed\n",
      "400-th minibatch\tloss: 0.110419\n",
      "22532.97874903679 secs elapsed\n",
      "600-th minibatch\tloss: 0.168653\n",
      "22617.32130765915 secs elapsed\n",
      "800-th minibatch\tloss: 0.224931\n",
      "22701.725680589676 secs elapsed\n",
      "1000-th minibatch\tloss: 0.274697\n",
      "22786.18473291397 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  54\tloss: 54.939404\n",
      "22786.184883356094 secs elapsed\n",
      "Saving weights to weights/resnet_v2-54epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.054091\n",
      "22870.641990423203 secs elapsed\n",
      "400-th minibatch\tloss: 0.107062\n",
      "22955.101430892944 secs elapsed\n",
      "600-th minibatch\tloss: 0.161938\n",
      "23039.464461803436 secs elapsed\n",
      "800-th minibatch\tloss: 0.217538\n",
      "23123.898346185684 secs elapsed\n",
      "1000-th minibatch\tloss: 0.270132\n",
      "23208.32697558403 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  55\tloss: 54.026364\n",
      "23208.327075719833 secs elapsed\n",
      "Saving weights to weights/resnet_v2-55epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.052653\n",
      "23292.79109787941 secs elapsed\n",
      "400-th minibatch\tloss: 0.107252\n",
      "23377.14975476265 secs elapsed\n",
      "600-th minibatch\tloss: 0.161748\n",
      "23461.51160621643 secs elapsed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800-th minibatch\tloss: 0.211088\n",
      "23545.906116485596 secs elapsed\n",
      "1000-th minibatch\tloss: 0.265667\n",
      "23630.32190132141 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  56\tloss: 53.133361\n",
      "23630.322004318237 secs elapsed\n",
      "Saving weights to weights/resnet_v2-56epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.051077\n",
      "23714.76241159439 secs elapsed\n",
      "400-th minibatch\tloss: 0.105874\n",
      "23799.14624261856 secs elapsed\n",
      "600-th minibatch\tloss: 0.156253\n",
      "23883.488463163376 secs elapsed\n",
      "800-th minibatch\tloss: 0.208023\n",
      "23967.85254764557 secs elapsed\n",
      "1000-th minibatch\tloss: 0.261174\n",
      "24052.23149752617 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  57\tloss: 52.234721\n",
      "24052.23162150383 secs elapsed\n",
      "Saving weights to weights/resnet_v2-57epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.050435\n",
      "24136.650026082993 secs elapsed\n",
      "400-th minibatch\tloss: 0.102245\n",
      "24221.02795982361 secs elapsed\n",
      "600-th minibatch\tloss: 0.152212\n",
      "24305.286283493042 secs elapsed\n",
      "800-th minibatch\tloss: 0.204559\n",
      "24389.654420375824 secs elapsed\n",
      "1000-th minibatch\tloss: 0.256772\n",
      "24474.06575369835 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  58\tloss: 51.354493\n",
      "24474.065855264664 secs elapsed\n",
      "Saving weights to weights/resnet_v2-58epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.047638\n",
      "24558.487018585205 secs elapsed\n",
      "400-th minibatch\tloss: 0.098140\n",
      "24642.869814634323 secs elapsed\n",
      "600-th minibatch\tloss: 0.150635\n",
      "24727.26711845398 secs elapsed\n",
      "800-th minibatch\tloss: 0.202649\n",
      "24811.639824151993 secs elapsed\n",
      "1000-th minibatch\tloss: 0.252484\n",
      "24895.965525865555 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  59\tloss: 50.496780\n",
      "24895.965659856796 secs elapsed\n",
      "Saving weights to weights/resnet_v2-59epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.048927\n",
      "24980.39767932892 secs elapsed\n",
      "400-th minibatch\tloss: 0.097958\n",
      "25064.804705381393 secs elapsed\n",
      "600-th minibatch\tloss: 0.150706\n",
      "25149.17254805565 secs elapsed\n",
      "800-th minibatch\tloss: 0.200992\n",
      "25233.63037919998 secs elapsed\n",
      "1000-th minibatch\tloss: 0.248316\n",
      "25318.031472682953 secs elapsed\n",
      "Learning rate: 0.0005\n",
      "epoch  60\tloss: 49.663268\n",
      "25318.03157544136 secs elapsed\n",
      "Saving weights to weights/resnet_v2-60epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.048381\n",
      "25402.443611383438 secs elapsed\n",
      "400-th minibatch\tloss: 0.099008\n",
      "25486.85336470604 secs elapsed\n",
      "600-th minibatch\tloss: 0.148583\n",
      "25571.22555565834 secs elapsed\n",
      "800-th minibatch\tloss: 0.196570\n",
      "25655.637257814407 secs elapsed\n",
      "1000-th minibatch\tloss: 0.244271\n",
      "25740.071873426437 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  61\tloss: 48.854128\n",
      "25740.07197713852 secs elapsed\n",
      "Saving weights to weights/resnet_v2-61epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.047637\n",
      "25824.561491250992 secs elapsed\n",
      "400-th minibatch\tloss: 0.095912\n",
      "25908.932733297348 secs elapsed\n",
      "600-th minibatch\tloss: 0.143840\n",
      "25993.26834011078 secs elapsed\n",
      "800-th minibatch\tloss: 0.190724\n",
      "26077.67603969574 secs elapsed\n",
      "1000-th minibatch\tloss: 0.239998\n",
      "26162.045014619827 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  62\tloss: 47.999694\n",
      "26162.045117139816 secs elapsed\n",
      "Saving weights to weights/resnet_v2-62epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.047406\n",
      "26246.404853105545 secs elapsed\n",
      "400-th minibatch\tloss: 0.095853\n",
      "26330.747920513153 secs elapsed\n",
      "600-th minibatch\tloss: 0.146450\n",
      "26415.13300561905 secs elapsed\n",
      "800-th minibatch\tloss: 0.193319\n",
      "26499.52468585968 secs elapsed\n",
      "1000-th minibatch\tloss: 0.239106\n",
      "26583.901261806488 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  63\tloss: 47.821105\n",
      "26583.90136551857 secs elapsed\n",
      "Saving weights to weights/resnet_v2-63epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045384\n",
      "26668.28624677658 secs elapsed\n",
      "400-th minibatch\tloss: 0.094185\n",
      "26752.684280395508 secs elapsed\n",
      "600-th minibatch\tloss: 0.142363\n",
      "26837.104107379913 secs elapsed\n",
      "800-th minibatch\tloss: 0.190654\n",
      "26921.4333422184 secs elapsed\n",
      "1000-th minibatch\tloss: 0.238332\n",
      "27005.837039232254 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  64\tloss: 47.666397\n",
      "27005.837189912796 secs elapsed\n",
      "Saving weights to weights/resnet_v2-64epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045520\n",
      "27090.250064849854 secs elapsed\n",
      "400-th minibatch\tloss: 0.093053\n",
      "27174.627001047134 secs elapsed\n",
      "600-th minibatch\tloss: 0.139826\n",
      "27259.028537988663 secs elapsed\n",
      "800-th minibatch\tloss: 0.187134\n",
      "27343.390196323395 secs elapsed\n",
      "1000-th minibatch\tloss: 0.237497\n",
      "27427.748372793198 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  65\tloss: 47.499439\n",
      "27427.74879002571 secs elapsed\n",
      "Saving weights to weights/resnet_v2-65epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.044621\n",
      "27512.087720155716 secs elapsed\n",
      "400-th minibatch\tloss: 0.090877\n",
      "27596.49444437027 secs elapsed\n",
      "600-th minibatch\tloss: 0.138816\n",
      "27680.834989786148 secs elapsed\n",
      "800-th minibatch\tloss: 0.188730\n",
      "27765.284032583237 secs elapsed\n",
      "1000-th minibatch\tloss: 0.236702\n",
      "27849.696048736572 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  66\tloss: 47.340394\n",
      "27849.6961517334 secs elapsed\n",
      "Saving weights to weights/resnet_v2-66epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046438\n",
      "27934.181374549866 secs elapsed\n",
      "400-th minibatch\tloss: 0.093855\n",
      "28018.59591650963 secs elapsed\n",
      "600-th minibatch\tloss: 0.141320\n",
      "28102.938222646713 secs elapsed\n",
      "800-th minibatch\tloss: 0.190716\n",
      "28187.32260775566 secs elapsed\n",
      "1000-th minibatch\tloss: 0.235881\n",
      "28271.687824964523 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  67\tloss: 47.176214\n",
      "28271.68793272972 secs elapsed\n",
      "Saving weights to weights/resnet_v2-67epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.048918\n",
      "28356.115761995316 secs elapsed\n",
      "400-th minibatch\tloss: 0.096611\n",
      "28440.517341852188 secs elapsed\n",
      "600-th minibatch\tloss: 0.141033\n",
      "28524.93826150894 secs elapsed\n",
      "800-th minibatch\tloss: 0.187782\n",
      "28609.359641075134 secs elapsed\n",
      "1000-th minibatch\tloss: 0.235092\n",
      "28693.741312980652 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  68\tloss: 47.018495\n",
      "28693.741723060608 secs elapsed\n",
      "Saving weights to weights/resnet_v2-68epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046243\n",
      "28778.18146133423 secs elapsed\n",
      "400-th minibatch\tloss: 0.092500\n",
      "28862.95988202095 secs elapsed\n",
      "600-th minibatch\tloss: 0.137181\n",
      "28947.373610258102 secs elapsed\n",
      "800-th minibatch\tloss: 0.184270\n",
      "29031.792407989502 secs elapsed\n",
      "1000-th minibatch\tloss: 0.234300\n",
      "29116.19369149208 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  69\tloss: 46.859947\n",
      "29116.19379544258 secs elapsed\n",
      "Saving weights to weights/resnet_v2-69epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.049304\n",
      "29200.65319275856 secs elapsed\n",
      "400-th minibatch\tloss: 0.094072\n",
      "29285.03059363365 secs elapsed\n",
      "600-th minibatch\tloss: 0.141618\n",
      "29369.428159236908 secs elapsed\n",
      "800-th minibatch\tloss: 0.186357\n",
      "29453.821541309357 secs elapsed\n",
      "1000-th minibatch\tloss: 0.233505\n",
      "29538.213785409927 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  70\tloss: 46.700918\n",
      "29538.213888645172 secs elapsed\n",
      "Saving weights to weights/resnet_v2-70epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.044883\n",
      "29622.653138399124 secs elapsed\n",
      "400-th minibatch\tloss: 0.090025\n",
      "29706.975714445114 secs elapsed\n",
      "600-th minibatch\tloss: 0.138857\n",
      "29791.356101989746 secs elapsed\n",
      "800-th minibatch\tloss: 0.185882\n",
      "29875.733005046844 secs elapsed\n",
      "1000-th minibatch\tloss: 0.232714\n",
      "29960.09535908699 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  71\tloss: 46.542787\n",
      "29960.095495224 secs elapsed\n",
      "Saving weights to weights/resnet_v2-71epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045648\n",
      "30044.58405804634 secs elapsed\n",
      "400-th minibatch\tloss: 0.093587\n",
      "30129.006564855576 secs elapsed\n",
      "600-th minibatch\tloss: 0.139376\n",
      "30213.43218421936 secs elapsed\n",
      "800-th minibatch\tloss: 0.185238\n",
      "30297.849536895752 secs elapsed\n",
      "1000-th minibatch\tloss: 0.231938\n",
      "30382.22299337387 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  72\tloss: 46.387640\n",
      "30382.223095178604 secs elapsed\n",
      "Saving weights to weights/resnet_v2-72epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046024\n",
      "30466.69851732254 secs elapsed\n",
      "400-th minibatch\tloss: 0.091817\n",
      "30551.03714299202 secs elapsed\n",
      "600-th minibatch\tloss: 0.136418\n",
      "30635.402697086334 secs elapsed\n",
      "800-th minibatch\tloss: 0.184585\n",
      "30719.804928064346 secs elapsed\n",
      "1000-th minibatch\tloss: 0.231157\n",
      "30804.21497106552 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  73\tloss: 46.231348\n",
      "30804.215072155 secs elapsed\n",
      "Saving weights to weights/resnet_v2-73epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046850\n",
      "30888.711190223694 secs elapsed\n",
      "400-th minibatch\tloss: 0.092500\n",
      "30973.03091287613 secs elapsed\n",
      "600-th minibatch\tloss: 0.140156\n",
      "31057.430223226547 secs elapsed\n",
      "800-th minibatch\tloss: 0.185917\n",
      "31141.101788520813 secs elapsed\n",
      "1000-th minibatch\tloss: 0.230391\n",
      "31225.5658288002 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  74\tloss: 46.078165\n",
      "31225.56596636772 secs elapsed\n",
      "Saving weights to weights/resnet_v2-74epoch.pth\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200-th minibatch\tloss: 0.045781\n",
      "31309.98774266243 secs elapsed\n",
      "400-th minibatch\tloss: 0.090508\n",
      "31394.32215833664 secs elapsed\n",
      "600-th minibatch\tloss: 0.135126\n",
      "31478.72291326523 secs elapsed\n",
      "800-th minibatch\tloss: 0.183593\n",
      "31563.16175341606 secs elapsed\n",
      "1000-th minibatch\tloss: 0.229630\n",
      "31647.579665660858 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  75\tloss: 45.925988\n",
      "31647.579767227173 secs elapsed\n",
      "Saving weights to weights/resnet_v2-75epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045970\n",
      "31732.019272089005 secs elapsed\n",
      "400-th minibatch\tloss: 0.091682\n",
      "31816.464767456055 secs elapsed\n",
      "600-th minibatch\tloss: 0.137188\n",
      "31900.890671491623 secs elapsed\n",
      "800-th minibatch\tloss: 0.185044\n",
      "31985.282541036606 secs elapsed\n",
      "1000-th minibatch\tloss: 0.228837\n",
      "32069.682838201523 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  76\tloss: 45.767320\n",
      "32069.682941436768 secs elapsed\n",
      "Saving weights to weights/resnet_v2-76epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.044655\n",
      "32154.15345954895 secs elapsed\n",
      "400-th minibatch\tloss: 0.092136\n",
      "32238.57156729698 secs elapsed\n",
      "600-th minibatch\tloss: 0.137660\n",
      "32322.96469950676 secs elapsed\n",
      "800-th minibatch\tloss: 0.181102\n",
      "32407.35830640793 secs elapsed\n",
      "1000-th minibatch\tloss: 0.228072\n",
      "32491.757990837097 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  77\tloss: 45.614497\n",
      "32491.758098840714 secs elapsed\n",
      "Saving weights to weights/resnet_v2-77epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045662\n",
      "32576.206322669983 secs elapsed\n",
      "400-th minibatch\tloss: 0.091180\n",
      "32660.631318092346 secs elapsed\n",
      "600-th minibatch\tloss: 0.137361\n",
      "32745.043725013733 secs elapsed\n",
      "800-th minibatch\tloss: 0.184637\n",
      "32829.456629276276 secs elapsed\n",
      "1000-th minibatch\tloss: 0.227325\n",
      "32913.90402126312 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  78\tloss: 45.465099\n",
      "32913.90412449837 secs elapsed\n",
      "Saving weights to weights/resnet_v2-78epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.044843\n",
      "32998.34085083008 secs elapsed\n",
      "400-th minibatch\tloss: 0.088480\n",
      "33082.68118786812 secs elapsed\n",
      "600-th minibatch\tloss: 0.133102\n",
      "33166.98003578186 secs elapsed\n",
      "800-th minibatch\tloss: 0.178537\n",
      "33251.36848497391 secs elapsed\n",
      "1000-th minibatch\tloss: 0.226569\n",
      "33335.798109054565 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  79\tloss: 45.313765\n",
      "33335.79821753502 secs elapsed\n",
      "Saving weights to weights/resnet_v2-79epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046376\n",
      "33420.21404361725 secs elapsed\n",
      "400-th minibatch\tloss: 0.092408\n",
      "33504.582046985626 secs elapsed\n",
      "600-th minibatch\tloss: 0.137042\n",
      "33588.96620965004 secs elapsed\n",
      "800-th minibatch\tloss: 0.182890\n",
      "33673.322588443756 secs elapsed\n",
      "1000-th minibatch\tloss: 0.225811\n",
      "33757.73686027527 secs elapsed\n",
      "Learning rate: 0.0001\n",
      "epoch  80\tloss: 45.162268\n",
      "33757.73696756363 secs elapsed\n",
      "Saving weights to weights/resnet_v2-80epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.043593\n",
      "33842.18247509003 secs elapsed\n",
      "400-th minibatch\tloss: 0.090639\n",
      "33926.62808728218 secs elapsed\n",
      "600-th minibatch\tloss: 0.136193\n",
      "34011.036024570465 secs elapsed\n",
      "800-th minibatch\tloss: 0.179309\n",
      "34095.43117213249 secs elapsed\n",
      "1000-th minibatch\tloss: 0.225059\n",
      "34179.8982899189 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  81\tloss: 45.011778\n",
      "34179.898396253586 secs elapsed\n",
      "Saving weights to weights/resnet_v2-81epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046808\n",
      "34264.26427721977 secs elapsed\n",
      "400-th minibatch\tloss: 0.088922\n",
      "34348.7185716629 secs elapsed\n",
      "600-th minibatch\tloss: 0.135250\n",
      "34433.12162709236 secs elapsed\n",
      "800-th minibatch\tloss: 0.180799\n",
      "34517.00367832184 secs elapsed\n",
      "1000-th minibatch\tloss: 0.224255\n",
      "34600.487251996994 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  82\tloss: 44.850933\n",
      "34600.48737883568 secs elapsed\n",
      "Saving weights to weights/resnet_v2-82epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.044792\n",
      "34684.63183045387 secs elapsed\n",
      "400-th minibatch\tloss: 0.087500\n",
      "34769.039803266525 secs elapsed\n",
      "600-th minibatch\tloss: 0.132539\n",
      "34853.41872358322 secs elapsed\n",
      "800-th minibatch\tloss: 0.177124\n",
      "34937.823828697205 secs elapsed\n",
      "1000-th minibatch\tloss: 0.223888\n",
      "35021.6389298439 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  83\tloss: 44.777657\n",
      "35021.639090299606 secs elapsed\n",
      "Saving weights to weights/resnet_v2-83epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.043713\n",
      "35105.20862698555 secs elapsed\n",
      "400-th minibatch\tloss: 0.091278\n",
      "35188.748908281326 secs elapsed\n",
      "600-th minibatch\tloss: 0.137571\n",
      "35272.314113378525 secs elapsed\n",
      "800-th minibatch\tloss: 0.181011\n",
      "35355.87625670433 secs elapsed\n",
      "1000-th minibatch\tloss: 0.223514\n",
      "35439.616706609726 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  84\tloss: 44.702863\n",
      "35439.61681103706 secs elapsed\n",
      "Saving weights to weights/resnet_v2-84epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046004\n",
      "35524.10992980003 secs elapsed\n",
      "400-th minibatch\tloss: 0.089468\n",
      "35608.493228435516 secs elapsed\n",
      "600-th minibatch\tloss: 0.136127\n",
      "35692.89065337181 secs elapsed\n",
      "800-th minibatch\tloss: 0.179444\n",
      "35777.22687101364 secs elapsed\n",
      "1000-th minibatch\tloss: 0.223150\n",
      "35861.60414099693 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  85\tloss: 44.630073\n",
      "35861.604244709015 secs elapsed\n",
      "Saving weights to weights/resnet_v2-85epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045035\n",
      "35946.08013677597 secs elapsed\n",
      "400-th minibatch\tloss: 0.089973\n",
      "36030.661707639694 secs elapsed\n",
      "600-th minibatch\tloss: 0.135324\n",
      "36115.67819857597 secs elapsed\n",
      "800-th minibatch\tloss: 0.178977\n",
      "36200.11837339401 secs elapsed\n",
      "1000-th minibatch\tloss: 0.222779\n",
      "36284.574944257736 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  86\tloss: 44.555890\n",
      "36284.575587034225 secs elapsed\n",
      "Saving weights to weights/resnet_v2-86epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.045648\n",
      "36369.03556704521 secs elapsed\n",
      "400-th minibatch\tloss: 0.090606\n",
      "36453.71429133415 secs elapsed\n",
      "600-th minibatch\tloss: 0.134709\n",
      "36538.0647315979 secs elapsed\n",
      "800-th minibatch\tloss: 0.177758\n",
      "36622.475675582886 secs elapsed\n",
      "1000-th minibatch\tloss: 0.222408\n",
      "36706.90093231201 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  87\tloss: 44.481501\n",
      "36706.901034832 secs elapsed\n",
      "Saving weights to weights/resnet_v2-87epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.041763\n",
      "36791.283792972565 secs elapsed\n",
      "400-th minibatch\tloss: 0.086836\n",
      "36875.64310145378 secs elapsed\n",
      "600-th minibatch\tloss: 0.132248\n",
      "36959.9414229393 secs elapsed\n",
      "800-th minibatch\tloss: 0.178238\n",
      "37044.290566682816 secs elapsed\n",
      "1000-th minibatch\tloss: 0.222043\n",
      "37128.77332282066 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  88\tloss: 44.408589\n",
      "37128.77342629433 secs elapsed\n",
      "Saving weights to weights/resnet_v2-88epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.046118\n",
      "37213.25418448448 secs elapsed\n",
      "400-th minibatch\tloss: 0.091663\n",
      "37297.65560603142 secs elapsed\n",
      "600-th minibatch\tloss: 0.135276\n",
      "37382.06961464882 secs elapsed\n",
      "800-th minibatch\tloss: 0.180232\n",
      "37466.42477107048 secs elapsed\n",
      "1000-th minibatch\tloss: 0.221669\n",
      "37550.78657698631 secs elapsed\n",
      "Learning rate: 5e-05\n",
      "epoch  89\tloss: 44.333726\n",
      "37550.78674864769 secs elapsed\n",
      "Saving weights to weights/resnet_v2-89epoch.pth\n",
      "\n",
      "200-th minibatch\tloss: 0.042270\n",
      "37635.858595609665 secs elapsed\n",
      "400-th minibatch\tloss: 0.084618\n",
      "37721.269927978516 secs elapsed\n",
      "600-th minibatch\tloss: 0.129684\n",
      "37806.37494158745 secs elapsed\n",
      "800-th minibatch\tloss: 0.175087\n",
      "37890.84642505646 secs elapsed\n"
     ]
    }
   ],
   "source": [
    "# based on https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "criterion = nn.CrossEntropyLoss()                   # categorical crossentropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_scheduler)\n",
    "\n",
    "# train loop\n",
    "T_start = time.time()\n",
    "for epoch in range(epochs):\n",
    "    print()\n",
    "    running_loss = 0.0\n",
    "    running_loss_mini = 0.0\n",
    "    for i, (inputs, targets) in enumerate(trainloader):\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # remove the channel dimension\n",
    "        targets = torch.argmax(targets, dim=1) # convert to (N, H, W)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        running_loss_mini += loss.item()\n",
    "\n",
    "        if i % T_print == T_print-1:\n",
    "            T_end = time.time()\n",
    "            print('%d-th minibatch\\tloss: %f' % (i+1, running_loss/T_print))\n",
    "            print(T_end-T_start, 'secs elapsed')\n",
    "            running_loss_mini = 0.0\n",
    "\n",
    "    # call scheduler every epoch\n",
    "    scheduler.step()\n",
    "\n",
    "    # print statistics\n",
    "    T_end = time.time()\n",
    "    print('epoch %3d\\tloss: %f' % (epoch + 1, running_loss))\n",
    "    print(T_end-T_start, 'secs elapsed')\n",
    "\n",
    "    # save\n",
    "    if True:#epoch % T_save == T_save-1:\n",
    "        weight_fname = f'resnet_v2-{epoch+1}epoch.pth'\n",
    "        save_model(model, weights_dir, weight_fname)\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "WUDgyW_ksUAQ",
    "outputId": "8a945ea1-ded8-41ef-ae2d-c78575723b7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving weights to drive/My Drive/coe197f/weights/resnet_v2-4th-epoch.pth\n"
     ]
    }
   ],
   "source": [
    "epoch = 3\n",
    "weight_fname = f'resnet_v2-{epoch+1}th-epoch.pth'\n",
    "save_model(model, weights_dir, weight_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "cySRLzrXtjcs",
    "outputId": "ef5d4c71-c95c-4895-8679-28a4246c0e7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 59M\n",
      "-rw------- 1 root root 59M May 29 09:14 resnet_v2-4th-epoch.pth\n"
     ]
    }
   ],
   "source": [
    "!ls -lht 'drive/My Drive/coe197f/weights'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "niwfzTkWrAeN"
   },
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "4Z4dBogbrAFP",
    "outputId": "cd929f98-5cc0-418c-d982-b7a4e03c7a89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256\n"
     ]
    }
   ],
   "source": [
    "# N,C,H,W = 1, 3, 480, 640\n",
    "N,C,H,W = 2,3,128,128\n",
    "backbone = build_resnet(input_shape=(N,C,H,W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "oLvAl_CerW1x",
    "outputId": "2857e20a-650a-4a18-c813-84f770c26864"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------x.shape = torch.Size([2, 3, 128, 128])\n",
      "torch.Size([2, 256, 32, 32])\n",
      "torch.Size([2, 256, 16, 16])\n",
      "torch.Size([2, 512, 8, 8])\n",
      "torch.Size([2, 512, 4, 4])\n",
      "torch.Size([2, 512, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "r_in = torch.zeros(N,C,H,W)\n",
    "b_out = backbone(r_in)\n",
    "for b in b_out:\n",
    "    print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ya08VoyBBsY"
   },
   "outputs": [],
   "source": [
    "model = fcn(r_in.shape, backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "yr4fR7KHBoai",
    "outputId": "dc2e4f5a-ece1-4b83-eea0-78debb800cc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------x.shape = torch.Size([2, 3, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:2973: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 128, 128])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:89: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    }
   ],
   "source": [
    "out = model(r_in)\n",
    "print(out.shape)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "d80KZvYkIZwt"
   ],
   "name": "197F-project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
